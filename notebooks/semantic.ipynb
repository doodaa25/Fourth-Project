

# 1) Install dependencies
!pip install -q sentence-transformers

# 2) Imports
import re
import numpy as np
import torch
import matplotlib.pyplot as plt
from sentence_transformers import SentenceTransformer, util

# 3) FILENAMES 
book1_path = "The Project Gutenberg eBook of Anna Karenina, by Leo Tolstoy.txt"
book2_path = "The_Project_Gutenberg_eBook_of_War_and_Peace,_by_Leo_Tolstoy_1.txt"

book1_label = "Anna Karenina"
book2_label = "War and Peace"

print("Using the following files:")
print(" -", book1_path)
print(" -", book2_path)

# 4) Load books
def load_book(path):
    with open(path, "r", encoding="utf-8", errors="ignore") as f:
        return f.read()

book1_raw = load_book(book1_path)
book2_raw = load_book(book2_path)

# 5) Strip Gutenberg header/footer (simple heuristic)
def strip_gutenberg(text):
    start = re.search(r"\*\*\* START OF .*? \*\*\*", text)
    if start:
        text = text[start.end():]

    end = re.search(r"\*\*\* END OF .*? \*\*\*", text)
    if end:
        text = text[:end.start()]

    return text.strip()

book1_clean = strip_gutenberg(book1_raw)
book2_clean = strip_gutenberg(book2_raw)

# 6) Split into paragraphs
def split_into_paragraphs(text, min_len=40):
    text = text.replace("\r\n", "\n").replace("\r", "\n")
    paras = re.split(r"\n\s*\n", text)
    paras = [p.strip() for p in paras if len(p.strip()) >= min_len]
    return paras

book1_paragraphs = split_into_paragraphs(book1_clean)
book2_paragraphs = split_into_paragraphs(book2_clean)

print("\nParagraphs extracted:")
print(book1_label, ":", len(book1_paragraphs))
print(book2_label, ":", len(book2_paragraphs))

# 7) Sentence splitter (simple regex)
sentence_split_regex = re.compile(
    r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?|!)\s+'
)

def split_into_sentences(paragraph):
    sents = sentence_split_regex.split(paragraph.strip())
    return [s.strip() for s in sents if s.strip()]

# 8) Load MiniLM sentence embedding model
model = SentenceTransformer("all-MiniLM-L6-v2")

# 9) Paragraph coherence function
def paragraph_coherence(paragraph, min_sentences=2):
    sentences = split_into_sentences(paragraph)
    sentences = [s for s in sentences if s]

    if len(sentences) < min_sentences:
        return None  # not enough sentences

    with torch.no_grad():
        emb = model.encode(sentences, convert_to_tensor=True)

    centroid = emb.mean(dim=0, keepdim=True)
    cos_sims = util.cos_sim(emb, centroid).squeeze(1)

    return float(cos_sims.mean().cpu().item())

# Optional helper if you ever need just the score
def coherence_score(paragraph):
    return paragraph_coherence(paragraph)

# 10) Compute coherence scores for paragraphs in each book
def compute_coherence(paragraphs, max_n=400):
    scores = []
    paras = []

    for i, p in enumerate(paragraphs):
        if i >= max_n:
            break
        score = paragraph_coherence(p)
        if score is not None:
            scores.append(score)
            paras.append(p)

    return np.array(scores), paras

print("\nComputing coherence scores (MiniLM)...")
book1_scores, book1_used = compute_coherence(book1_paragraphs, max_n=400)
book2_scores, book2_used = compute_coherence(book2_paragraphs, max_n=400)
print("Done.")

# 11) Descriptive statistics
def describe(scores, label):
    print(f"\n=== {label} ===")
    print("Paragraphs:", len(scores))
    print("Mean:", float(np.mean(scores)))
    print("Std:", float(np.std(scores)))
    print("Min:", float(np.min(scores)))
    print("Max:", float(np.max(scores)))

describe(book1_scores, book1_label)
describe(book2_scores, book2_label)

# 12) IMPROVED VISUALIZATION
# Two separate histograms with the same x-axis & mean lines

all_scores = np.concatenate([book1_scores, book2_scores])
bins = np.linspace(all_scores.min(), all_scores.max(), 25)

fig, axs = plt.subplots(2, 1, figsize=(9, 7), sharex=True)

# Anna Karenina
axs[0].hist(book1_scores, bins=bins)
axs[0].axvline(book1_scores.mean(), linestyle="--", linewidth=2,
               label=f"Mean = {book1_scores.mean():.3f}")
axs[0].set_title("Paragraph Coherence – Anna Karenina")
axs[0].set_ylabel("Frequency")
axs[0].legend(loc="upper left")

# War and Peace
axs[1].hist(book2_scores, bins=bins)
axs[1].axvline(book2_scores.mean(), linestyle="--", linewidth=2,
               label=f"Mean = {book2_scores.mean():.3f}")
axs[1].set_title("Paragraph Coherence – War and Peace")
axs[1].set_xlabel("Semantic Coherence Score")
axs[1].set_ylabel("Frequency")
axs[1].legend(loc="upper left")

plt.tight_layout()
plt.show()

# 13) Show most/least coherent paragraphs (optional, for interpretation)
def show_extremes(scores, paras, label, k=3):
    idx = np.argsort(scores)

    print(f"\n--- Top {k} MOST coherent ({label}) ---\n")
    for i in idx[-k:][::-1]:
        print(f"Score: {scores[i]:.4f}\n")
        print(paras[i][:1000], "\n")
        print("-" * 60)

    print(f"\n--- Top {k} LEAST coherent ({label}) ---\n")
    for i in idx[:k]:
        print(f"Score: {scores[i]:.4f}\n")
        print(paras[i][:1000], "\n")
        print("-" * 60)

show_extremes(book1_scores, book1_used, book1_label)
show_extremes(book2_scores, book2_used, book2_label)

print("\nAll tasks completed successfully!")
